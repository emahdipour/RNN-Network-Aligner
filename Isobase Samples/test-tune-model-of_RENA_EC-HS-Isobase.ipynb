{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This project implemented by Elham Mahdipour\n",
    "## She is a Ph.D. Candidate of computer engineering at Yazd University, Yazd, Iran.\n",
    "### Please feel free and contact to me: elham.mahdipour@gmail.com/ elham.mahdipour@stu.yazd.ac.ir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 \n",
    "## Create Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x2bff6888780>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "G1=nx.read_weighted_edgelist('large dataset\\ec-ec.evals')\n",
    "G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x2bff71517f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G2=nx.read_weighted_edgelist('large dataset\\hs-hs.evals')\n",
    "G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265\n",
      "22232\n"
     ]
    }
   ],
   "source": [
    "### Check and Swap if G1 > G2 ###\n",
    "if len(G1)>len(G2):\n",
    "    temp=G1\n",
    "    G1=G2\n",
    "    G2=temp\n",
    "print(len(G1))\n",
    "print(len(G2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x2bff4ad6668>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_target_na=nx.read_weighted_edgelist('large dataset\\ec-hs.evals')\n",
    "G_target_na  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed1=G1.edges()\n",
    "ed2=G2.edges()\n",
    "\n",
    "nd1=G1.nodes()\n",
    "nd2=G2.nodes()\n",
    "\n",
    "el1=list(ed1)\n",
    "el2=list(ed2)\n",
    "\n",
    "nd1=list(nd1)\n",
    "nd2=list(nd2)\n",
    "\n",
    "degG1 = [val for (node, val) in G1.degree()]\n",
    "degG2 = [val for (node, val) in G2.degree()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute score for create similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deg_Diff(G1,G2):\n",
    "    Degree_Difference=np.zeros((len(G1),len(G2)))\n",
    "    for i in range(len(G1)):\n",
    "        for j in range(len(G2)):\n",
    "            Degree_Difference[i][j]=abs(degG1[i]-degG2[j])/max(degG1[i],degG2[j])\n",
    "    return Degree_Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_pageRank(X):\n",
    "    a=nx.pagerank(X)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficient_pagerank(x,y):  #x is G1, y is G2\n",
    "   # print(len(x))\n",
    "    p1=score_pageRank(x)\n",
    "    b=p1.values()\n",
    "    pr1=list(b)\n",
    "    p2=score_pageRank(y)\n",
    "    c=p2.values()\n",
    "    pr2=list(c)\n",
    "    pr=np.zeros((len(x),len(y)))\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(y)):\n",
    "            #print(pr1[i],pr2[j])\n",
    "            pr[i][j]=abs(pr1[i]-pr2[j])/max(pr1[i],pr2[j])   #minimum pr is maximum similarity of topology \n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficient_edges(index_node, G, GraphNumber):\n",
    "    if GraphNumber==1:\n",
    "        sum_edge=0        \n",
    "        for i in G.neighbors(nd1[index_node]):                       \n",
    "            sum_edge=sum_edge+degG1[nd1.index(i)]\n",
    "        #print(sum_edge)\n",
    "        temp=(degG1[index_node]-1) if degG1[index_node]> 1 else 1        \n",
    "        coeff_node=(2*sum_edge)/(degG1[index_node]*temp)\n",
    "    else:\n",
    "        sum_edge=0        \n",
    "        for i in G.neighbors(nd2[index_node]):                       \n",
    "            sum_edge=sum_edge+degG2[nd2.index(i)]\n",
    "        #print(sum_edge)\n",
    "        temp=(degG2[index_node]-1) if degG2[index_node]> 1 else 1        \n",
    "        coeff_node=(2*sum_edge)/(degG2[index_node]*temp)\n",
    "    return coeff_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Ea(G1,G2):\n",
    "    Ea_G1=np.zeros(len(G1))\n",
    "    Ea_G2=np.zeros(len(G2))\n",
    "    for i in range(len(G1)):\n",
    "        Ea_G1[i]=coefficient_edges(i, G1, 1)\n",
    "    for j in range(len(G2)):\n",
    "        Ea_G2[j]=coefficient_edges(j,G2,2)\n",
    "    ea=[Ea_G1, Ea_G2]\n",
    "    return(ea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute relative clustering coefficient difference between node a (in G1) and node b (in G2)\n",
    "def CD(G1, G2):\n",
    "    cd=np.zeros((len(G1),len(G2)))\n",
    "    EA=compute_Ea(G1,G2)\n",
    "    #print(EA[0])      #Ea for G1\n",
    "    #print(\"===================\")\n",
    "    #print(EA[1])      #Ea for G2\n",
    "    for i in range(len(G1)):\n",
    "        for j in range(len(G2)):\n",
    "            cd[i,j]=abs(EA[0][i]-EA[1][j])/max(EA[0][i],EA[1][j])\n",
    "    return cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_score(x,y):\n",
    "    seq=np.zeros((len(x),len(y)))\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(y)):\n",
    "            q1=G_target_na.get_edge_data(str(nd1[i]),str(nd2[j]))\n",
    "            if q1==None:\n",
    "                c=0\n",
    "            else:\n",
    "                c=list(q1.values())\n",
    "                c=c[0]\n",
    "            seq[i][j]=c    \n",
    "            \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(G1,G2):\n",
    "    coeff_pr=coefficient_pagerank(G1,G2)\n",
    "    dd=deg_Diff(G1,G2)\n",
    "    cd=CD(G1,G2)\n",
    "    seq_sc=sequence_score(G1,G2)\n",
    "    \n",
    "    alpha=0.1\n",
    "    betta=0.2\n",
    "    gamma=0.2\n",
    "    zetta=1-alpha-betta-gamma\n",
    "    s=alpha*(1-coeff_pr)+betta*(1-dd)+gamma*(1-cd)+zetta*seq_sc\n",
    "    return s,coeff_pr, dd, cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "m, coeff_pr, dd, cd=compute_score(G1,G2)\n",
    "sim=m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create data for deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change problem to classification \n",
    "## [node of G1, node of G2, BLAST, Coefficient page rank, clustering coefficient difference,  similarity score, alignment=yes(1) or no(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "int_nd1=np.zeros(len(nd1))\n",
    "int_nd2=np.zeros(len(nd2))\n",
    "\n",
    "species=['ec','sc','ce','dm','mm','hs']\n",
    "ch1=0  #please set index for first species of species list, for example index of ec is 0\n",
    "ch2=5  #please set index for second species of species list, for example index of hs is 5\n",
    "# If don't set index with considering species may be given an error\n",
    "\n",
    "for i in range(len(nd1)):\n",
    "    if (species[ch1] in nd1[i] or species[ch2] in nd1[i]):\n",
    "        s=nd1[i][2:]\n",
    "        x=int(s)\n",
    "        int_nd1[i]=x    \n",
    "for i in range(len(nd2)):\n",
    "    if (species[ch1] in nd2[i] or species[ch2] in nd2[i]):\n",
    "        s=nd2[i][2:]\n",
    "        x=int(s)\n",
    "        int_nd2[i]=x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94819480\n"
     ]
    }
   ],
   "source": [
    "# en_mat is encoding matrix\n",
    "en_mat=[]\n",
    "\n",
    "for i in range(len(nd1)):\n",
    "    for j in range(len(nd2)):\n",
    "        if G_target_na.has_edge(nd1[i],nd2[j]):\n",
    "            align_class='Yes'\n",
    "        else:\n",
    "            align_class='No'\n",
    "        \n",
    "        sample=[int_nd1[i],int_nd2[j], coeff_pr[i][j], dd[i][j],cd[i][j],sim[i][j],align_class]\n",
    "        en_mat.append(sample)\n",
    "print(len(en_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8770 94810710\n"
     ]
    }
   ],
   "source": [
    "yc=[]\n",
    "noc=[]\n",
    "for i in range(len(en_mat)):\n",
    "    if en_mat[i][6]=='Yes':\n",
    "        yc.append(en_mat[i])\n",
    "    else:\n",
    "        noc.append(en_mat[i])\n",
    "print(len(yc), len(noc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30008770, 30008770)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=yc+sort_noc[0:30000000] \n",
    "len(data)\n",
    "X=[]\n",
    "y=[]\n",
    "for i in range(len(data)):\n",
    "    X.append(data[i][0:6])\n",
    "    y.append(data[i][6])\n",
    "\n",
    "len(X),len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27007893 27007893 3000877 3000877\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('large dataset/ec-hs-data.pickle', 'wb') as f:\n",
    "    pickle.dump([X_train, y_train,X_test,y_test],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('large dataset/ec-hs-data.pickle','rb') as f:\n",
    "    X_train, y_train,X_test,y_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27032462 27032462 3003607 3003607\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr,y_tr,x_te,y_te=X_train, y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Label encode Class (Species)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "# One Hot Encode\n",
    "y_train = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode Class (Species)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "encoded_Y = encoder.transform(y_test)\n",
    "# One Hot Encode\n",
    "y_test = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "X_test=np.array(X_test)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: RENA Network \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tune RENA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model_rnn = load_model('deep_model_resample_6features_rnn_ec-sc.h5')\n",
    "model_rnn.load_weights('deep_model_resample_6features_rnn_weights_ec-sc.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27007893/27007893 [==============================] - 2499s 93us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0015723015683626396, 1.0, 0.0015785049181431532, 2.4786654648778494e-06]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without resample\n",
    "result_tr = model_rnn.evaluate(X_train, y_train)\n",
    "result_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033],\n",
       "       ...,\n",
       "       [0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "out_tr=model_rnn.predict(X_train)\n",
    "out_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27000005        0]\n",
      " [       0     7888]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_train.argmax(axis=1), out_tr.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "print(precision_score(y_train.argmax(axis=1), out_tr.argmax(axis=1) , average=\"macro\"))\n",
    "print(recall_score(y_train.argmax(axis=1), out_tr.argmax(axis=1) , average=\"macro\"))\n",
    "print(f1_score(y_train.argmax(axis=1), out_tr.argmax(axis=1) , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000877/3000877 [==============================] - 247s 82us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0015723059410443707, 1.0, 0.0015719984658062458, 2.4665660021128133e-06]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without resample\n",
    "result_te = model_rnn.evaluate(X_test, y_test)\n",
    "result_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033],\n",
       "       ...,\n",
       "       [0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "out_te=model_rnn.predict(X_test)\n",
    "out_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2999995       0]\n",
      " [      0     882]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test.argmax(axis=1), out_te.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "print(precision_score(y_test.argmax(axis=1), out_te.argmax(axis=1) , average=\"macro\"))\n",
    "print(recall_score(y_test.argmax(axis=1), out_te.argmax(axis=1) , average=\"macro\"))\n",
    "print(f1_score(y_test.argmax(axis=1), out_te.argmax(axis=1) , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test other classifier without resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tra,y_tra,x_tes,y_tes=X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train,X_test,y_test=x_tr,y_tr,x_te,y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LDA classifier on training set: 1.00\n",
      "Accuracy of LDA classifier on test set: 1.00\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "print('Accuracy of LDA classifier on training set: {:.2f}'\n",
    "     .format(lda.score(X_train, y_train)))\n",
    "print('Accuracy of LDA classifier on test set: {:.2f}'\n",
    "     .format(lda.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9999966676408263\n",
      "[[27000005        0]\n",
      " [      90     7798]]\n",
      "0.9999983333391975\n",
      "0.9942951318458417\n",
      "0.9971303664642064\n",
      "Wall time: 4min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix\n",
    "# Make predictions\n",
    "preds_tr = lda.predict(X_train)\n",
    "print(preds_tr)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_train, preds_tr))\n",
    "\n",
    "print(confusion_matrix(y_train, preds_tr))\n",
    "print(precision_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(recall_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(f1_score(y_train, preds_tr , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9999973341126611\n",
      "[[2999995       0]\n",
      " [      8     874]]\n",
      "0.9999986666680001\n",
      "0.9954648526077097\n",
      "0.9977214290050929\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix\n",
    "# Make predictions\n",
    "preds = lda.predict(X_test)\n",
    "print(preds)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_test, preds))\n",
    "\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(precision_score(y_test, preds , average=\"macro\"))\n",
    "print(recall_score(y_test, preds , average=\"macro\"))\n",
    "print(f1_score(y_test, preds , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN classifier on training set: 1.00\n",
      "Accuracy of KNN classifier on test set: 1.00\n",
      "Wall time: 3h 35min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "print('Accuracy of KNN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))\n",
    "print('Accuracy of KNN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9998747031275463\n",
      "[[27000005        0]\n",
      " [    3510     4378]]\n",
      "0.9999373413461971\n",
      "0.7868480739872145\n",
      "0.8645219834169753\n",
      "Wall time: 3h 21min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,accuracy_score\n",
    "# Make predictions\n",
    "preds_tr = knn.predict(X_train)\n",
    "print(preds_tr)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_train, preds_tr))\n",
    "\n",
    "print(confusion_matrix(y_train, preds_tr))\n",
    "print(precision_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(recall_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(f1_score(y_train, preds_tr , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9998700385146987\n",
      "[[2999995        0]\n",
      " [    376      506]]\n",
      "0.9999350084169723\n",
      "0.777510142786848147\n",
      "0.856889074864521931\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix,accuracy_score\n",
    "# Make predictions\n",
    "preds =  knn.predict(X_test)\n",
    "print(preds)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_test, preds))\n",
    "\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(precision_score(y_test, preds , average=\"macro\"))\n",
    "print(recall_score(y_test, preds , average=\"macro\"))\n",
    "print(f1_score(y_test, preds , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on training set: 0.58\n",
      "Accuracy of SVM classifier on test set: 0.58\n",
      "Wall time: 10h 44min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "print('Accuracy of SVM classifier on training set: {:.2f}'\n",
    "     .format(svm.score(X_train, y_train)))\n",
    "print('Accuracy of SVM classifier on test set: {:.2f}'\n",
    "     .format(svm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.5889524961954863\n",
      "[[15898478   11101527]\n",
      " [       0       7888]]\n",
      "0.500355014356181691\n",
      "0.794416205264268464\n",
      "0.371316523841365972\n",
      "Wall time: 10h 33min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,accuracy_score\n",
    "# Make predictions\n",
    "preds_tr = svm.predict(X_train)\n",
    "print(preds_tr)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_train, preds_tr))\n",
    "\n",
    "print(confusion_matrix(y_train, preds_tr))\n",
    "print(precision_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(recall_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(f1_score(y_train, preds_tr , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.5890181443697415\n",
      "[[1766689    1233306]\n",
      " [      0        882]]\n",
      "0.5003573262145774\n",
      "0.7944486574832169\n",
      "0.3713468388568891\n",
      "Wall time: 14min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,accuracy_score\n",
    "# Make predictions\n",
    "preds = svm.predict(X_test)\n",
    "print(preds)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_test, preds))\n",
    "\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(precision_score(y_test, preds , average=\"macro\"))\n",
    "print(recall_score(y_test, preds , average=\"macro\"))\n",
    "print(f1_score(y_test, preds , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
