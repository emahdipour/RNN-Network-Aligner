{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This project implemented by Elham Mahdipour\n",
    "## She is a Ph.D. Candidate of computer engineering at Yazd University, Yazd, Iran.\n",
    "## Please feel free and contact to me: elham.mahdipour@gmail.com/ elham.mahdipour@stu.yazd.ac.ir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 \n",
    "## Create Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x2819f0f86a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "G1=nx.read_weighted_edgelist('large dataset\\sc-sc.evals')\n",
    "G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x281a1ad0278>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G2=nx.read_weighted_edgelist('large dataset\\ec-ec.evals')\n",
    "G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265\n",
      "6613\n"
     ]
    }
   ],
   "source": [
    "### Check and Swap if G1 > G2 ###\n",
    "if len(G1)>len(G2):\n",
    "    temp=G1\n",
    "    G1=G2\n",
    "    G2=temp\n",
    "print(len(G1))\n",
    "print(len(G2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x281a2126588>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_target_na=nx.read_weighted_edgelist('large dataset\\ec-sc.evals')\n",
    "G_target_na  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed1=G1.edges()\n",
    "ed2=G2.edges()\n",
    "\n",
    "nd1=G1.nodes()\n",
    "nd2=G2.nodes()\n",
    "\n",
    "el1=list(ed1)\n",
    "el2=list(ed2)\n",
    "\n",
    "nd1=list(nd1)\n",
    "nd2=list(nd2)\n",
    "\n",
    "degG1 = [val for (node, val) in G1.degree()]\n",
    "degG2 = [val for (node, val) in G2.degree()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute score for create similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deg_Diff(G1,G2):\n",
    "    Degree_Difference=np.zeros((len(G1),len(G2)))\n",
    "    for i in range(len(G1)):\n",
    "        for j in range(len(G2)):\n",
    "            Degree_Difference[i][j]=abs(degG1[i]-degG2[j])/max(degG1[i],degG2[j])\n",
    "    return Degree_Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_pageRank(X):\n",
    "    a=nx.pagerank(X)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficient_pagerank(x,y):  #x is G1, y is G2\n",
    "   # print(len(x))\n",
    "    p1=score_pageRank(x)\n",
    "    b=p1.values()\n",
    "    pr1=list(b)\n",
    "    p2=score_pageRank(y)\n",
    "    c=p2.values()\n",
    "    pr2=list(c)\n",
    "    pr=np.zeros((len(x),len(y)))\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(y)):\n",
    "            #print(pr1[i],pr2[j])\n",
    "            pr[i][j]=abs(pr1[i]-pr2[j])/max(pr1[i],pr2[j])   #minimum pr is maximum similarity of topology \n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficient_edges(index_node, G, GraphNumber):\n",
    "    if GraphNumber==1:\n",
    "        sum_edge=0        \n",
    "        for i in G.neighbors(nd1[index_node]):                       \n",
    "            sum_edge=sum_edge+degG1[nd1.index(i)]\n",
    "        #print(sum_edge)\n",
    "        temp=(degG1[index_node]-1) if degG1[index_node]> 1 else 1        \n",
    "        coeff_node=(2*sum_edge)/(degG1[index_node]*temp)\n",
    "    else:\n",
    "        sum_edge=0        \n",
    "        for i in G.neighbors(nd2[index_node]):                       \n",
    "            sum_edge=sum_edge+degG2[nd2.index(i)]\n",
    "        #print(sum_edge)\n",
    "        temp=(degG2[index_node]-1) if degG2[index_node]> 1 else 1        \n",
    "        coeff_node=(2*sum_edge)/(degG2[index_node]*temp)\n",
    "    return coeff_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Ea(G1,G2):\n",
    "    Ea_G1=np.zeros(len(G1))\n",
    "    Ea_G2=np.zeros(len(G2))\n",
    "    for i in range(len(G1)):\n",
    "        Ea_G1[i]=coefficient_edges(i, G1, 1)\n",
    "    for j in range(len(G2)):\n",
    "        Ea_G2[j]=coefficient_edges(j,G2,2)\n",
    "    ea=[Ea_G1, Ea_G2]\n",
    "    return(ea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute relative clustering coefficient difference between node a (in G1) and node b (in G2)\n",
    "def CD(G1, G2):\n",
    "    cd=np.zeros((len(G1),len(G2)))\n",
    "    EA=compute_Ea(G1,G2)\n",
    "    #print(EA[0])      #Ea for G1\n",
    "    #print(\"===================\")\n",
    "    #print(EA[1])      #Ea for G2\n",
    "    for i in range(len(G1)):\n",
    "        for j in range(len(G2)):\n",
    "            cd[i,j]=abs(EA[0][i]-EA[1][j])/max(EA[0][i],EA[1][j])\n",
    "    return cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_score(x,y):\n",
    "    seq=np.zeros((len(x),len(y)))\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(y)):\n",
    "            q1=G_target_na.get_edge_data(str(nd1[i]),str(nd2[j]))\n",
    "            if q1==None:\n",
    "                c=0\n",
    "            else:\n",
    "                c=list(q1.values())\n",
    "                c=c[0]\n",
    "            seq[i][j]=c    \n",
    "            \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(G1,G2):\n",
    "    coeff_pr=coefficient_pagerank(G1,G2)\n",
    "    dd=deg_Diff(G1,G2)\n",
    "    cd=CD(G1,G2)\n",
    "    seq_sc=sequence_score(G1,G2)\n",
    "    \n",
    "    alpha=0.1\n",
    "    betta=0.2\n",
    "    gamma=0.2\n",
    "    zetta=1-alpha-betta-gamma\n",
    "    s=alpha*(1-coeff_pr)+betta*(1-dd)+gamma*(1-cd)+zetta*seq_sc\n",
    "    return s,coeff_pr, dd, cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "m, coeff_pr, dd, cd=compute_score(G1,G2)\n",
    "sim=m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create data for deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change problem to classification \n",
    "## [node of G1, node of G2, Coefficient page rank, clustering coefficient difference,  similarity score, alignment=yes(1) or no(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "int_nd1=np.zeros(len(nd1))\n",
    "int_nd2=np.zeros(len(nd2))\n",
    "\n",
    "species=['ec','sc','ce','dm','mm','hs']\n",
    "ch1=0  #please select index for first species of species list, for example index of ec is 0\n",
    "ch2=1  #please select index for second species of species list, for example index of sc is 1\n",
    "for i in range(len(nd1)):\n",
    "    if (species[ch1] in nd1[i] or species[ch2] in nd1[i]):\n",
    "        s=nd1[i][2:]\n",
    "        x=int(s)\n",
    "        int_nd1[i]=x    \n",
    "for i in range(len(nd2)):\n",
    "    if (species[ch1] in nd2[i] or species[ch2] in nd2[i]):\n",
    "        s=nd2[i][2:]\n",
    "        x=int(s)\n",
    "        int_nd2[i]=x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28204445\n"
     ]
    }
   ],
   "source": [
    "# en_mat is encoding matrix\n",
    "en_mat=[]\n",
    "\n",
    "for i in range(len(nd1)):\n",
    "    for j in range(len(nd2)):\n",
    "        if G_target_na.has_edge(nd1[i],nd2[j]):\n",
    "            align_class='Yes'\n",
    "        else:\n",
    "            align_class='No'\n",
    "        \n",
    "        sample=[int_nd1[i],int_nd2[j], coeff_pr[i][j], dd[i][j],cd[i][j],sim[i][j],align_class] \n",
    "        en_mat.append(sample)\n",
    "print(len(en_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4561 28199884\n"
     ]
    }
   ],
   "source": [
    "yc=[]\n",
    "noc=[]\n",
    "for i in range(len(en_mat)):\n",
    "    if en_mat[i][6]=='Yes':\n",
    "        yc.append(en_mat[i])\n",
    "    else:\n",
    "        noc.append(en_mat[i])\n",
    "print(len(yc), len(noc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28204445, 28204445)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=yc+noc \n",
    "len(data)\n",
    "X=[]\n",
    "y=[]\n",
    "for i in range(len(data)):\n",
    "    X.append(data[i][0:6])\n",
    "    y.append(data[i][6])\n",
    "\n",
    "len(X),len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('large dataset/ec-sc-data.pickle', 'wb') as f:\n",
    "    pickle.dump([X_train, y_train,X_test,y_test],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('large dataset/ec-sc-data.pickle','rb') as f:\n",
    "    X_train, y_train,X_test,y_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25384000 25384000 2820445 2820445\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr,y_tr,x_te,y_te=X_train, y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve data Imbalance problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({'No': 25379873, 'Yes': 4127})\n",
      "Resampled dataset shape Counter({'No': 25379873, 'Yes': 25379873})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "print('Original dataset shape %s' % Counter(y_train))\n",
    "#sampling_strategy = 'not minority'\n",
    "smt = SMOTEENN(random_state=42) #sampling_strategy=sampling_strategy)\n",
    "\n",
    "X_res_train, y_res_train = smt.fit_resample(X_train, y_train)\n",
    "print('Resampled dataset shape %s' % Counter(y_res_train))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({'No': 2820011, 'Yes': 434})\n",
      "Resampled dataset shape Counter({'No': 2820011, 'Yes': 2820011})\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset shape %s' % Counter(y_test))\n",
    "smt = SMOTEENN(random_state=42)\n",
    "X_res_test, y_res_test = smt.fit_resample(X_test, y_test)\n",
    "print('Resampled dataset shape %s' % Counter(y_res_test))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_res_train=np.array(X_res_train)\n",
    "y_res_train=np.array(y_res_train)\n",
    "X_res_test=np.array(X_res_test)\n",
    "y_res_test=np.array(y_res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode Class (Species)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_res_train)\n",
    "encoded_Y = encoder.transform(y_res_train)\n",
    "# One Hot Encode\n",
    "y_res_train = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode Class (Species)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_res_test)\n",
    "encoded_Y = encoder.transform(y_res_test)\n",
    "# One Hot Encode\n",
    "y_res_test = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Label encode Class (Species)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "# One Hot Encode\n",
    "y_train = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode Class (Species)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "encoded_Y = encoder.transform(y_test)\n",
    "# One Hot Encode\n",
    "y_test = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "X_test=np.array(X_test)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Architecture of RENA Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, None, 8)           48        \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 4)                 52        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 110\n",
      "Trainable params: 110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40607796 samples, validate on 10151950 samples\n",
      "Epoch 1/2\n",
      "40607796/40607796 [==============================] - 5902s 145us/step - loss: 2.2543e-04 - acc: 1.0000 - mae: -6.2950e-06 - mse: 3.0406e-05 - val_loss: 1.4782e-05 - val_acc: 1.0000 - val_mae: 1.4785e-05 - val_mse: 2.1852e-10\n",
      "Epoch 2/2\n",
      "40607796/40607796 [==============================] - 5904s 145us/step - loss: 2.6877e-04 - acc: 1.0000 - mae: 2.6924e-04 - mse: 5.8059e-07 - val_loss: 6.3419e-05 - val_acc: 1.0000 - val_mae: 6.3132e-05 - val_mse: 4.0305e-09\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense,SimpleRNN,Embedding\n",
    "import keras\n",
    "\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='acc',patience=1,),\n",
    "keras.callbacks.ModelCheckpoint(filepath='my_model.h5',monitor='val_loss',save_best_only=True,)]\n",
    "#del model_rnn\n",
    "model_rnn = Sequential()\n",
    "\n",
    "model_rnn.add(layers.Embedding(6, 8))\n",
    "model_rnn.add(layers.SimpleRNN(4))\n",
    "model_rnn.add(Dense(2, activation='softmax'))\n",
    "model_rnn.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['acc','mae','mse'])\n",
    "model_rnn.summary()\n",
    "history_rnn = model_rnn.fit(X_res_train, y_res_train,epochs=2,validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import h5py\n",
    "model_rnn.save('large dataset/deep_model_resample_6features_rnn_ec-sc.h5')\n",
    "model_rnn.save_weights('large dataset/deep_model_resample_6features_rnn_weights_ec-sc.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_rnn = load_model('deep_model_resample_6features_rnn_ec-sc.h5')\n",
    "model_rnn.load_weights('deep_model_resample_6features_rnn_weights_ec-sc.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25384000/25384000 [==============================] - 1465s 58us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_tr = model_rnn.evaluate(X_train, y_train)\n",
    "results_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "output_tr=model_rnn.predict(X_train)\n",
    "output_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25379886        0]\n",
      " [       0     4114]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_train.argmax(axis=1), output_tr.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "print(precision_score(y_train.argmax(axis=1), output_tr.argmax(axis=1) , average=\"macro\"))\n",
    "print(recall_score(y_train.argmax(axis=1), output_tr.argmax(axis=1) , average=\"macro\"))\n",
    "print(f1_score(y_train.argmax(axis=1), output_tr.argmax(axis=1) , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2820445/2820445 [==============================] - 165s 59us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_te = model_rnn.evaluate(X_test, y_test)\n",
    "results_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2819998       0]\n",
      " [      0     447]]\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_te=model_rnn.predict(X_test)\n",
    "#print(output_te)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test.argmax(axis=1), output_te.argmax(axis=1)))\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "print(precision_score(y_test.argmax(axis=1), output_te.argmax(axis=1) , average=\"macro\"))\n",
    "print(recall_score(y_test.argmax(axis=1), output_te.argmax(axis=1) , average=\"macro\"))\n",
    "print(f1_score(y_test.argmax(axis=1), output_te.argmax(axis=1) , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RENA model for real data (without resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, None, 8)           48        \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 4)                 52        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 110\n",
      "Trainable params: 110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20307200 samples, validate on 5076800 samples\n",
      "Epoch 1/2\n",
      "20307200/20307200 [==============================] - 2932s 144us/step - loss: 8.6748e-04 - acc: 0.9999 - mae: 3.0912e-04 - mse: 1.1521e-04 - val_loss: 5.0945e-05 - val_acc: 1.0000 - val_mae: 5.0020e-05 - val_mse: 1.7195e-06\n",
      "Epoch 2/2\n",
      "20307200/20307200 [==============================] - 2949s 145us/step - loss: 3.0630e-04 - acc: 1.0000 - mae: 2.8873e-04 - mse: 2.2891e-05 - val_loss: 1.2872e-04 - val_acc: 1.0000 - val_mae: 1.1416e-04 - val_mse: 2.1657e-05\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense,SimpleRNN,Embedding\n",
    "import keras\n",
    "\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='acc',patience=1,),\n",
    "keras.callbacks.ModelCheckpoint(filepath='my_model.h5',monitor='val_loss',save_best_only=True,)]\n",
    "#del model_rnn\n",
    "model_rnn = Sequential()\n",
    "\n",
    "model_rnn.add(layers.Embedding(6, 8))\n",
    "model_rnn.add(layers.SimpleRNN(4))\n",
    "model_rnn.add(Dense(2, activation='softmax'))\n",
    "model_rnn.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['acc','mae','mse'])\n",
    "model_rnn.summary()\n",
    "history_rnn = model_rnn.fit(X_train, y_train,epochs=2,validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import h5py\n",
    "model_rnn.save('large dataset/deep_model_before_resample_6features_rnn_ec-sc.h5')\n",
    "model_rnn.save_weights('large dataset/deep_model_before_resample_6features_rnn_weights_ec-sc.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test RNN before resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#del model\n",
    "model_rnn = load_model('large dataset/deep_model_before_resample_6features_rnn_ec-sc.h5')\n",
    "model_rnn.load_weights('large dataset/deep_model_before_resample_6features_rnn_weights_ec-sc.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25384000/25384000 [==============================] - 1460s 58us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[nan, 0.99989253282547, nan, nan]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_tr = model_rnn.evaluate(X_train, y_train)\n",
    "results_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan, nan],\n",
       "       [nan, nan],\n",
       "       [nan, nan],\n",
       "       ...,\n",
       "       [nan, nan],\n",
       "       [nan, nan],\n",
       "       [nan, nan]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "output_tr=model_rnn.predict(X_train)\n",
    "output_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25379886        0]\n",
      " [    4114        0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_train.argmax(axis=1), output_tr.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4999189647021746\n",
      "0.5\n",
      "0.49995947906746147\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "print(precision_score(y_train.argmax(axis=1), output_tr.argmax(axis=1) , average=\"macro\"))\n",
    "print(recall_score(y_train.argmax(axis=1), output_tr.argmax(axis=1) , average=\"macro\"))\n",
    "print(f1_score(y_train.argmax(axis=1), output_tr.argmax(axis=1) , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2820445/2820445 [==============================] - 162s 57us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[nan, 0.9998415112495422, nan, nan]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_te = model_rnn.evaluate(X_test, y_test)\n",
    "results_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2819998       0]\n",
      " [    447       0]]\n",
      "0.49992075718547957\n",
      "0.5\n",
      "0.49996037545277916\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_te=model_rnn.predict(X_test)\n",
    "#print(output_te)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test.argmax(axis=1), output_te.argmax(axis=1)))\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "print(precision_score(y_test.argmax(axis=1), output_te.argmax(axis=1) , average=\"macro\"))\n",
    "print(recall_score(y_test.argmax(axis=1), output_te.argmax(axis=1) , average=\"macro\"))\n",
    "print(f1_score(y_test.argmax(axis=1), output_te.argmax(axis=1) , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test other classifier without resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tra,y_tra,x_tes,y_tes=X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train,X_test,y_test=x_tr,y_tr,x_te,y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LDA classifier on training set: 1.00\n",
      "Accuracy of LDA classifier on test set: 1.00\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "print('Accuracy of LDA classifier on training set: {:.2f}'\n",
    "     .format(lda.score(X_train, y_train)))\n",
    "print('Accuracy of LDA classifier on test set: {:.2f}'\n",
    "     .format(lda.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9999761660888749\n",
      "[[25379886        0]\n",
      " [     605     3509]]\n",
      "0.9999880813968492\n",
      "0.9264705882352942\n",
      "0.9603115009448575\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix\n",
    "# Make predictions\n",
    "preds_tr = lda.predict(X_train)\n",
    "print(preds_tr)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_train, preds_tr))\n",
    "\n",
    "print(confusion_matrix(y_train, preds_tr))\n",
    "print(precision_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(recall_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(f1_score(y_train, preds_tr , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9999726993435433\n",
      "[[2819998       0]\n",
      " [     77     370]]\n",
      "0.9999863478808187\n",
      "0.9138702460850112\n",
      "0.952869550836202\n",
      "Wall time: 25.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix\n",
    "# Make predictions\n",
    "preds = lda.predict(X_test)\n",
    "print(preds)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_test, preds))\n",
    "\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(precision_score(y_test, preds , average=\"macro\"))\n",
    "print(recall_score(y_test, preds , average=\"macro\"))\n",
    "print(f1_score(y_test, preds , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999230617863493\n",
      "[[2819998       0]\n",
      " [    217     230]]\n",
      "0.9999615277558627\n",
      "0.7572706935123042\n",
      "0.8397148842604457\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('large dataset/saved_ldamodel_ec-hp.pickle','rb') as f:\n",
    "    lda_from_pickle=pickle.load(f)\n",
    "preds=lda_from_pickle.predict(X_test)\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_test, preds))\n",
    "\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(precision_score(y_test, preds , average=\"macro\"))\n",
    "print(recall_score(y_test, preds , average=\"macro\"))\n",
    "print(f1_score(y_test, preds , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9999238890639773\n",
      "[[25379886        0]\n",
      " [    1932     2182]]\n",
      "0.9999619412604723\n",
      "0.7651920272241128\n",
      "0.8465502202236368\n"
     ]
    }
   ],
   "source": [
    "preds_tr = lda_from_pickle.predict(X_train)\n",
    "print(preds_tr)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_train, preds_tr))\n",
    "\n",
    "print(confusion_matrix(y_train, preds_tr))\n",
    "print(precision_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(recall_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(f1_score(y_train, preds_tr , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "# time: one hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9999808934762054\n",
      "[[25379886        0]\n",
      " [     485     3629]]\n",
      "0.9999904453721342\n",
      "0.9410549343704424\n",
      "0.9686766122826788\n",
      "Wall time: 20min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix\n",
    "# Make predictions\n",
    "preds_tr = knn.predict(X_train)\n",
    "print(preds_tr)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_train, preds_tr))\n",
    "\n",
    "print(confusion_matrix(y_train, preds_tr))\n",
    "print(precision_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(recall_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(f1_score(y_train, preds_tr , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9999709265736435\n",
      "[[2819998       0]\n",
      " [     82     365]]\n",
      "0.9999854614053503\n",
      "0.9082774049217002\n",
      "0.9495001197595498\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix,accuracy_score\n",
    "# Make predictions\n",
    "preds =  knn.predict(X_test)\n",
    "print(preds)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_test, preds))\n",
    "\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(precision_score(y_test, preds , average=\"macro\"))\n",
    "print(recall_score(y_test, preds , average=\"macro\"))\n",
    "print(f1_score(y_test, preds , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on training set: 1.00\n",
      "Accuracy of SVM classifier on test set: 1.00\n",
      "Wall time: 3h 29min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "print('Accuracy of SVM classifier on training set: {:.2f}'\n",
    "     .format(svm.score(X_train, y_train)))\n",
    "print('Accuracy of SVM classifier on test set: {:.2f}'\n",
    "     .format(svm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.99998262685156\n",
      "[[25379886        0]\n",
      " [     441     3673]]\n",
      "0.999991312168673\n",
      "0.9464025279533301\n",
      "0.9716792312360152\n",
      "Wall time: 42min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,accuracy_score\n",
    "# Make predictions\n",
    "preds_tr = svm.predict(X_train)\n",
    "print(preds_tr)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_train, preds_tr))\n",
    "\n",
    "print(confusion_matrix(y_train, preds_tr))\n",
    "print(precision_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(recall_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(f1_score(y_train, preds_tr , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9999840450709019\n",
      "[[2819998       0]\n",
      " [     45     402]]\n",
      "0.9999920213982553\n",
      "0.9496644295302014\n",
      "0.973494243882846\n",
      "Wall time: 4min 42s\n",
      "Parser   : 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,accuracy_score\n",
    "# Make predictions\n",
    "preds = svm.predict(X_test)\n",
    "print(preds)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_test, preds))\n",
    "\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(precision_score(y_test, preds , average=\"macro\"))\n",
    "print(recall_score(y_test, preds , average=\"macro\"))\n",
    "print(f1_score(y_test, preds , average=\"macro\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
